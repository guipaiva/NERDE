{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl_io\n",
    "import re\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../dataset/metadata/', exist_ok=True)\n",
    "os.makedirs('../dataset/all/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(doc):\n",
    "    def multiline_label(doc):\n",
    "        final_text = ''\n",
    "        prev_idx = 0\n",
    "        text = doc['data']\n",
    "        all_labels = sorted(doc['label'])\n",
    "\n",
    "        for start, end, _ in all_labels:\n",
    "            final_text += text[prev_idx:start]\n",
    "            prev_idx = end\n",
    "            ent_text = text[start:end].replace('\\n', ' ')\n",
    "            final_text += ent_text\n",
    "            \n",
    "        return final_text\n",
    "    roman_numerals = re.compile(r\"(^|\\s)([IVX]+\\.)\\n\", re.IGNORECASE)\n",
    "    number = re.compile(\"(^|\\s)(\\d\\.(?:\\d\\.*)*)\\n\")\n",
    "    \n",
    "    #fix multiline labels\n",
    "    text = multiline_label(doc)\n",
    "    #fix numberal numbers split\n",
    "    text = re.sub(roman_numerals, r\"\\1\\2 \", text)\n",
    "    #fix numbered itens split\n",
    "    text = re.sub(number, r\"\\1\\2 \", text)\n",
    "\n",
    "    doc['data'] = text\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = jsonl_io.read_jsonl('../dataset/annotated.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_to_conll(text, labels):\n",
    "    ents = []\n",
    "    prev_idx = 0\n",
    "    masked_text = \"\"\n",
    "    for start, end, label in labels:\n",
    "        prev_text = text[prev_idx:start]\n",
    "        masked_text += prev_text\n",
    "        masked_text += 'ENT'\n",
    "        ent_text = text[start:end]\n",
    "        tokenized_ent = word_tokenize(ent_text, language='portuguese', preserve_line=True)\n",
    "        prev_idx = end\n",
    "        iob_ents = []\n",
    "        for idx, tok in enumerate(tokenized_ent):\n",
    "            if not idx:\n",
    "                iob_ents.append(f'{tok} B-{label}')\n",
    "            else:\n",
    "                iob_ents.append(f'{tok} I-{label}')\n",
    "        ents.append(iob_ents)\n",
    "    last_chunk = text[prev_idx:]\n",
    "    masked_text += last_chunk\n",
    "    \n",
    "    final_text = \"\"\n",
    "    \n",
    "    ent_idx = 0\n",
    "    for line in masked_text.splitlines():\n",
    "        line_tokens = word_tokenize(line, language='portuguese', preserve_line=True)\n",
    "        for token in line_tokens:\n",
    "            if token == 'ENT':\n",
    "                final_text += '\\n'.join(ents[ent_idx])\n",
    "                final_text += '\\n'\n",
    "                ent_idx += 1\n",
    "            else:\n",
    "                final_text += f'{token} O\\n'\n",
    "        final_text += '\\n'\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  8.80it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(annotated_data):\n",
    "    n_sei = doc['n_sei']\n",
    "    metadata = {}\n",
    "    metadata['numero_sei'] = n_sei\n",
    "    metadata['origem'] = doc['origem']\n",
    "    metadata['tipo_documento'] = doc['tipo_documento']\n",
    "    metadata_filename = f'{n_sei}_meta.json'\n",
    "    conll_filename = f'{n_sei}.conll'\n",
    "    with open('../dataset/metadata/' + metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    fixed = post_process(doc)\n",
    "    text = fixed['data']\n",
    "    labels = sorted(fixed['label'])\n",
    "    conll = offset_to_conll(text, labels)\n",
    "    with open('../dataset/all/' + conll_filename, 'w') as f:\n",
    "        f.write(conll)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e48e09a6fa42eb9d7fcce203cae01b0981b0aec130f03992a3a4940edecbb98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
