{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl_io\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import json\n",
    "from nltk import word_tokenize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../dataset/metadata/', exist_ok=True)\n",
    "os.makedirs('../dataset/all/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_to_conll(text, labels):\n",
    "    ents = []\n",
    "    prev_idx = 0\n",
    "    masked_text = \"\"\n",
    "    for start, end, label in labels:\n",
    "        prev_text = text[prev_idx:start]\n",
    "        masked_text += prev_text\n",
    "        masked_text += 'ENT'\n",
    "        ent_text = text[start:end]\n",
    "        tokenized_ent = word_tokenize(ent_text, language='portuguese', preserve_line=True)\n",
    "        prev_idx = end\n",
    "        iob_ents = []\n",
    "        for idx, tok in enumerate(tokenized_ent):\n",
    "            if not idx:\n",
    "                iob_ents.append(f'{tok} B-{label}')\n",
    "            else:\n",
    "                iob_ents.append(f'{tok} I-{label}')\n",
    "        ents.append(iob_ents)\n",
    "    last_chunk = text[prev_idx:]\n",
    "    masked_text += last_chunk\n",
    "    \n",
    "    final_text = \"\"\n",
    "    \n",
    "    ent_idx = 0\n",
    "    for line in masked_text.splitlines():\n",
    "        line_tokens = word_tokenize(line, language='portuguese', preserve_line=True)\n",
    "        for token in line_tokens:\n",
    "            if token == 'ENT':\n",
    "                final_text += '\\n'.join(ents[ent_idx])\n",
    "                final_text += '\\n'\n",
    "                ent_idx += 1\n",
    "            else:\n",
    "                final_text += f'{token} O\\n'\n",
    "        final_text += '\\n'\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_data = jsonl_io.read_jsonl('../processed.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 35.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(annotated_data):\n",
    "    n_sei = doc['n_sei']\n",
    "    metadata = {}\n",
    "    metadata['numero_sei'] = n_sei\n",
    "    metadata['origem'] = doc['origem']\n",
    "    metadata['tipo_documento'] = doc['tipo_documento']\n",
    "    metadata_filename = f'{n_sei}_meta.json'\n",
    "    conll_filename = f'{n_sei}.conll'\n",
    "    with open('../dataset/metadata/' + metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f)\n",
    "    text = doc['data']\n",
    "    labels = sorted(doc['label'])\n",
    "    conll = offset_to_conll(text, labels)\n",
    "    with open('../dataset/all/' + conll_filename, 'w') as f:\n",
    "        f.write(conll)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e48e09a6fa42eb9d7fcce203cae01b0981b0aec130f03992a3a4940edecbb98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
